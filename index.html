<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jose Luis Ponton </title> <meta name="author" content="Jose Luis Ponton"> <meta name="description" content="Jose Luis Ponton Portfolio"> <meta name="keywords" content="Jose Luis, Ponton, Portfolio, Animation, Deep Learning, Computer Graphics, Virtual Reality, Mixed Reality, VR, MR, AR# add your own keywords or leave empty"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joseluisponton.com/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%73%65.%6C%75%69%73.%70%6F%6E%74%6F%6E@%75%70%63.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-6576-4528" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=yNa9qSIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Jose-Luis-Ponton/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/JLPM22" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jlponton" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_joseluis_ponton.pdf">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jose Luis</span> Ponton </h1> <p class="desc"><a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">UPC</a>. Computer Animation. Machine Learning. Computer Graphics. XR.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?3092f7ca5fe03aacacd3986d99d164c0" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hello! I am a Ph.D. student at <a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">Universitat Politècnica de Catalunya</a> in Barcelona (Spain), working at the <a href="https://www.virvig.eu/" rel="external nofollow noopener" target="_blank">ViRVIG</a> research group.</p> <p>My research interests are <strong>computer animation</strong>, <strong>virtual and augmented reality</strong>, <strong>deep learning</strong> and <strong>computer graphics</strong>.</p> <p>Currently, I am working in <strong>data-driven character animation</strong>, in particular, motion understanding and synthesis, retargeting, inverse kinematics, motion capture and virtual reality avatars.</p> <p>I received my M.Sc. in Computer Graphics and B.S. in Computer Science at <a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">Universitat Politècnica de Catalunya</a>.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 01, 2024</th> <td> I am starting an internship at <a href="https://www.via-center.science/" rel="external nofollow noopener" target="_blank">Max Planck Institute for Informatics</a> under the supervision of <a href="https://people.mpi-inf.mpg.de/~theobalt/" rel="external nofollow noopener" target="_blank">Prof. Dr. Christian Theobalt</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 30, 2024</th> <td> I’ve joined the core R&amp;D team at <a href="https://hyper.online/" rel="external nofollow noopener" target="_blank">Hyper Online</a>, a YCombinator and Amazon-backed startup, as an external contractor to continue developing virtual avatars for VTubers. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2024</th> <td> Our paper <a href="/assets/pdf/stretchyourreach_chi24.pdf">Stretch Your Reach</a> was accepted at the <a href="https://chi2024.acm.org/" rel="external nofollow noopener" target="_blank">CHI ‘24 conference on Human Factors in Computing Systems</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 16, 2024</th> <td> Our paper <a href="/assets/pdf/expectedcollisionfeedback_ieeevr24.pdf">Exploring the role of expected collision feedback in VR</a> was accepted at the <a href="https://ieeevr.org/2024/" rel="external nofollow noopener" target="_blank">31st IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 14, 2023</th> <td> I will be presenting in <a href="https://asia.siggraph.org/2023/" rel="external nofollow noopener" target="_blank">SIGGRAPH Asia 2023</a> our paper <a href="/assets/pdf/sparseposer_siggraphasia2023.pdf">SparsePoser</a> accepted at the <a href="https://doi.org/10.1145/3625264" rel="external nofollow noopener" target="_blank">ACM Transactions on Graphics</a> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 06, 2023</th> <td> Our paper <a href="/assets/pdf/fittedavatars_vr2023.pdf">Fitted avatars</a> was accepted at the <a href="https://link.springer.com/article/10.1007/s10055-023-00821-z" rel="external nofollow noopener" target="_blank">Springer Virtual Reality</a> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 30, 2023</th> <td> Our paper <a href="/assets/pdf/animation_fidelity_ieeevr23.pdf">Animation Fidelity in Self-Avatars</a> was accepted at the <a href="https://ieeevr.org/2023/" rel="external nofollow noopener" target="_blank">30th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2023)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 10, 2022</th> <td> I was awarded a Ph.D. scholarship <a href="https://www.universidades.gob.es/ayudas-para-la-formacion-de-profesorado-universitario-fpu-2021/" rel="external nofollow noopener" target="_blank">FPU 2021 </a> (code FPU21/01927) from the Spanish Administration! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 12, 2022</th> <td> Our paper <a href="/assets/pdf/mmvr_sca2022.pdf">Motion Matching for VR</a> was accepted at the <a href="https://computeranimation.org/2022/" rel="external nofollow noopener" target="_blank">21st annual ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA 2022)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2022</th> <td> I am starting an internship at <a href="https://www.cyens.org.cy/en-gb/" rel="external nofollow noopener" target="_blank">CYENS Centre of Excellence</a> under the supervision of <a href="http://andreasaristidou.com/" rel="external nofollow noopener" target="_blank">Dr. Andreas Aristidou</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 27, 2022</th> <td> Our paper <a href="/assets/pdf/avatarGo_shortEG2022.pdf">AvatarGo</a> about animating VR avatars was accepted at <a href="https://eg2022.univ-reims.fr/" rel="external nofollow noopener" target="_blank">Eurographics 2022</a>. </td> </tr> </table> </div> </div> <div style="margin-bottom: 20px;"></div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/stretchyourreach-480.webp 480w,/assets/img/publication_preview/stretchyourreach-800.webp 800w,/assets/img/publication_preview/stretchyourreach-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/stretchyourreach.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="stretchyourreach.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2024stretchyourreach" class="col-sm-8"> <div class="title">Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction</div> <div class="author"> <em>Jose Luis Ponton</em>, Reza Keshavarz, Alejandro Beacco, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In Proceedings of the CHI Conference on Human Factors in Computing Systems</em> , May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/stretchyourreach_chi24.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/e1-LuHWDjLU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>In VR, misalignment between the user’s body and their virtual avatar can disrupt the sense of embodiment and hinder interaction. This study examines five different interaction modes, manipulating factors like virtual controller visibility and hand positioning. The results reveal that "stretching" the avatar’s arms to align with the user’s hands significantly improves embodiment, proprioception, user preference, and task performance. Interestingly, whether the virtual controllers are rendered or not had no significant impact.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ponton2024stretchyourreach</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Keshavarz, Reza and Beacco, Alejandro and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703300}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613904.3642268}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613904.3642268}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{109}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{15}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '24}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/expectedcollision-480.webp 480w,/assets/img/publication_preview/expectedcollision-800.webp 800w,/assets/img/publication_preview/expectedcollision-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/expectedcollision.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="expectedcollision.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10494178" class="col-sm-8"> <div class="title">Exploring the Role of Expected Collision Feedback in Crowded Virtual Environments</div> <div class="author"> <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <em>Jose Luis Ponton</em>, Alejandro Beacco, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In 2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em> , Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/expectedcollisionfeedback_ieeevr24.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/DoG37fthIZE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>This study investigates the impact of expected collision feedback on user perception and interaction in virtual environments populated with virtual humans. We examine common feedback techniques (auditory, tactile) and the effect of inducing the expectation of a physical collision with a real person. Results indicate that expected collision feedback significantly influences both global navigation and local movements, as well as subjective perceptions of presence and copresence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10494178</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yun, Haoran and Ponton, Jose Luis and Beacco, Alejandro and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring the Role of Expected Collision Feedback in Crowded Virtual Environments}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{472-481}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VR58804.2024.00068}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sparseposer-480.webp 480w,/assets/img/publication_preview/sparseposer-800.webp 800w,/assets/img/publication_preview/sparseposer-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/sparseposer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sparseposer.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2023sparseposer" class="col-sm-8"> <div class="title">SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data</div> <div class="author"> <em>Jose Luis Ponton</em>, <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <a href="http://andreasaristidou.com/" rel="external nofollow noopener" target="_blank">Andreas Aristidou</a>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <em>In SIGGRAPH Asia 2023</em> <div class="periodical"> <em>ACM Trans. Graph.</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/sparseposer_siggraphasia2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/BAi4KoHtehY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/UPC-ViRVIG/SparsePoser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://upc-virvig.github.io/SparsePoser/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>SparsePoser is a novel deep learning-based approach for reconstructing full-body human motion in VR using a reduced set of six tracking devices. By learning the human motion manifold from motion capture data and incorporating a learned IK component, it overcomes the limitations of traditional IK methods and IMU-based approaches, producing high-quality, continuous poses. Our extensive evaluations on motion capture datasets and real-time demos demonstrate that SparsePoser outperforms state-of-the-art techniques and can be applied to users with varying body dimensions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2023sparseposer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Yun, Haoran and Aristidou, Andreas and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{February 2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia 2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0730-0301}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3625264}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3625264}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Trans. Graph.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fittedavatars-480.webp 480w,/assets/img/publication_preview/fittedavatars-800.webp 800w,/assets/img/publication_preview/fittedavatars-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fittedavatars.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fittedavatars.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2023fittedavatars" class="col-sm-8"> <div class="title">Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality</div> <div class="author"> <em>Jose Luis Ponton</em>, Victor Ceballos, Lesly Acosta, <a href="https://www.cs.upc.edu/~arios/" rel="external nofollow noopener" target="_blank">Alejandro Rios</a>, Eva Monclus, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>Virtual Reality</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/fittedavatars_vr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/U7KOhTGByyc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Self-avatars are essential for enhancing presence and communication in VR. However, ensuring accurate dimensions often requires manual measurements, which can be time-consuming and error-prone. This paper proposes an automatic method using a head-mounted display (HMD), hand controllers, and trackers to accurately estimate user measurements and adjust the virtual skeleton. This method improves the alignment of the avatar with the user’s body, surpassing solutions that rely on uniform scaling or assumptions about joint locations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2023fittedavatars</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Ceballos, Victor and Acosta, Lesly and Rios, Alejandro and Monclus, Eva and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Virtual Reality}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2541--2560}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1434-9957}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10055-023-00821-z}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/animationfidelity-480.webp 480w,/assets/img/publication_preview/animationfidelity-800.webp 800w,/assets/img/publication_preview/animationfidelity-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/animationfidelity.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="animationfidelity.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yun2023animationfidelity" class="col-sm-8"> <div class="title">Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency</div> <div class="author"> <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <em>Jose Luis Ponton</em>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em> , Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/animation_fidelity_ieeevr23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/X1POhdDIvm0?si=SnPHPOAxrmu3lERO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>This research investigates the impact of avatar animation fidelity on various tasks in VR. Comparing three animation techniques (two using Inverse Kinematics and one using a motion capture system), the study found that the quality of animation influences the user’s sense of embodiment. Surprisingly, IK-based solutions, despite using fewer sensors, sometimes outperformed MoCap in tasks requiring precise positioning due to lower latency and less positional drift.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yun2023animationfidelity</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yun, Haoran and Ponton, Jose Luis and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{286-296}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2642-5254}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VR55154.2023.00044}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmvr-480.webp 480w,/assets/img/publication_preview/mmvr-800.webp 800w,/assets/img/publication_preview/mmvr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmvr.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmvr.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2022mmvr" class="col-sm-8"> <div class="title">Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices</div> <div class="author"> <em>Jose Luis Ponton</em>, <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <em>In ACM SIGGRAPH / Eurographics Symposium on Computer Animation</em> <div class="periodical"> <em>Computer Graphics Forum</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/mmvr_sca2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/crU9oLX0GnM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/UPC-ViRVIG/MMVR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://upc-virvig.github.io/MMVR/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Current VR avatar animation methods suffer from inaccurate lower body movement due to the limited tracking data available from consumer-grade devices. This paper presents a novel approach that utilizes a neural network to estimate user body orientation from HMD and controller inputs. By combining this orientation data with HMD velocity and rotation, a feature vector is generated to drive a motion matching algorithm. Tested on a MoCap database of VR users, this system produces a wider range of lower body animations that precisely match user orientation, enabling more realistic and diverse movement representation in VR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2022mmvr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Yun, Haoran and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH / Eurographics Symposium on Computer Animation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Graphics Forum}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{107-118}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1467-8659}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/cgf.14628}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatargo-480.webp 480w,/assets/img/publication_preview/avatargo-800.webp 800w,/assets/img/publication_preview/avatargo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/avatargo.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="avatargo.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2022avatargo" class="col-sm-8"> <div class="title">AvatarGo: Plug and Play self-avatars for VR</div> <div class="author"> <em>Jose Luis Ponton</em>, Eva Monclus, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In Eurographics 2022 - Short Papers</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/avatarGo_shortEG2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/DWU4p-a-uXo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/UPC-ViRVIG/AvatarGo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Self-avatars in VR can greatly enhance user experience, especially in collaborative settings. Current methods for animating avatars can lead to mismatches between the user’s movement and the avatar’s. This paper introduces a simple but effective technique for calculating precise offsets for each user, resulting in significantly improved avatar movement and a stronger sense of embodiment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ponton2022avatargo</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Eurographics 2022 - Short Papers}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{AvatarGo: Plug and Play self-avatars for VR}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Monclus, Eva and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Eurographics Association}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1017-4656}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-03868-169-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.2312/egs.20221037}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Jose Luis Ponton. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MM0JZRGDZ0"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MM0JZRGDZ0");</script> </body> </html>