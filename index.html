<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jose Luis Ponton </title> <meta name="author" content="Jose Luis Ponton"> <meta name="description" content="Jose Luis Ponton Portfolio"> <meta name="keywords" content="Jose Luis, Ponton, Portfolio, Animation, Deep Learning, Computer Graphics, Virtual Reality, Mixed Reality, VR, MR, AR# add your own keywords or leave empty"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joseluisponton.com/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%73%65.%6C%75%69%73.%70%6F%6E%74%6F%6E@%75%70%63.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-6576-4528" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=yNa9qSIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Jose-Luis-Ponton/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/JLPM22" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jlponton" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv_joseluis_ponton.pdf">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jose Luis</span> Ponton </h1> <p class="desc"><a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">UPC</a>. Computer Animation. Machine Learning. Computer Graphics. XR.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?3092f7ca5fe03aacacd3986d99d164c0" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hello! I am a Ph.D. student at <a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">Universitat Politècnica de Catalunya</a> in Barcelona (Spain), working at the <a href="https://www.virvig.eu/" rel="external nofollow noopener" target="_blank">ViRVIG</a> research group.</p> <p>My research interests are <strong>computer animation</strong>, <strong>virtual and augmented reality</strong>, <strong>deep learning</strong> and <strong>computer graphics</strong>.</p> <p>Currently, I am working in <strong>data-driven character animation</strong>, in particular, motion understanding and synthesis, retargeting, inverse kinematics, motion capture and virtual reality avatars.</p> <p>I received my M.Sc. in Computer Graphics and B.S. in Computer Science at <a href="https://www.upc.edu/en" rel="external nofollow noopener" target="_blank">Universitat Politècnica de Catalunya</a>.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Sep 14, 2023</th> <td> I will be presenting in <a href="https://asia.siggraph.org/2023/" rel="external nofollow noopener" target="_blank">SIGGRAPH Asia 2023</a> our paper <a href="/assets/pdf/sparseposer_siggraphasia2023.pdf">SparsePoser</a> accepted at the <a href="https://doi.org/10.1145/3625264" rel="external nofollow noopener" target="_blank">ACM Transactions on Graphics</a> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 06, 2023</th> <td> Our paper <a href="/assets/pdf/fittedavatars_vr2023.pdf">Fitted avatars</a> was accepted at the <a href="https://link.springer.com/article/10.1007/s10055-023-00821-z" rel="external nofollow noopener" target="_blank">Springer Virtual Reality</a> journal. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 30, 2023</th> <td> Our paper <a href="/assets/pdf/animation_fidelity_ieeevr23.pdf">Animation Fidelity in Self-Avatars</a> was accepted at the <a href="https://ieeevr.org/2023/" rel="external nofollow noopener" target="_blank">30th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2023)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 10, 2022</th> <td> I was awarded a Ph.D. scholarship <a href="https://www.universidades.gob.es/ayudas-para-la-formacion-de-profesorado-universitario-fpu-2021/" rel="external nofollow noopener" target="_blank">FPU 2021 </a> (code FPU21/01927) from the Spanish Administration! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 12, 2022</th> <td> Our paper <a href="/assets/pdf/mmvr_sca2022.pdf">Motion Matching for VR</a> was accepted at the <a href="https://computeranimation.org/2022/" rel="external nofollow noopener" target="_blank">21st annual ACM SIGGRAPH / Eurographics Symposium on Computer Animation (SCA 2022)</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sparseposer-480.webp 480w,/assets/img/publication_preview/sparseposer-800.webp 800w,/assets/img/publication_preview/sparseposer-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/sparseposer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sparseposer.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2023sparseposer" class="col-sm-8"> <div class="title">SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data</div> <div class="author"> <em>Jose Luis Ponton</em>, <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <a href="http://andreasaristidou.com/" rel="external nofollow noopener" target="_blank">Andreas Aristidou</a>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <em>In SIGGRAPH Asia 2023</em> <div class="periodical"> <em>ACM Trans. Graph.</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/sparseposer_siggraphasia2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/UPC-ViRVIG/SparsePoser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://upc-virvig.github.io/SparsePoser/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions though, it is important to minimize the number of sensors attached to the subject’s body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2023sparseposer</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Yun, Haoran and Aristidou, Andreas and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{February 2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia 2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0730-0301}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3625264}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3625264}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Trans. Graph.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{14}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fittedavatars-480.webp 480w,/assets/img/publication_preview/fittedavatars-800.webp 800w,/assets/img/publication_preview/fittedavatars-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fittedavatars.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fittedavatars.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2023fittedavatars" class="col-sm-8"> <div class="title">Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality</div> <div class="author"> <em>Jose Luis Ponton</em>, Victor Ceballos, Lesly Acosta, <a href="https://www.cs.upc.edu/~arios/" rel="external nofollow noopener" target="_blank">Alejandro Rios</a>, Eva Monclus, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>Virtual Reality</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/fittedavatars_vr2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In the era of the metaverse, self-avatars are gaining popularity, as they can enhance presence and provide embodiment when a user is immersed in Virtual Reality. They are also very important in collaborative Virtual Reality to improve communication through gestures. Whether we are using a complex motion capture solution or a few trackers with inverse kinematics (IK), it is essential to have a good match in size between the avatar and the user, as otherwise mismatches in self-avatar posture could be noticeable for the user. To achieve such a correct match in dimensions, a manual process is often required, with the need for a second person to take measurements of body limbs and introduce them into the system. This process can be time-consuming, and prone to errors. In this paper, we propose an automatic measuring method that simply requires the user to do a small set of exercises while wearing a Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work provides an affordable and quick method to automatically extract user measurements and adjust the virtual humanoid skeleton to the exact dimensions. Our results show that our method can reduce the misalignment produced by the IK system when compared to other solutions that simply apply a uniform scaling to an avatar based on the height of the HMD, and make assumptions about the locations of joints with respect to the trackers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2023fittedavatars</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Ceballos, Victor and Acosta, Lesly and Rios, Alejandro and Monclus, Eva and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Virtual Reality}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2541--2560}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1434-9957}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{27}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10055-023-00821-z}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/animationfidelity-480.webp 480w,/assets/img/publication_preview/animationfidelity-800.webp 800w,/assets/img/publication_preview/animationfidelity-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/animationfidelity.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="animationfidelity.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yun2023animationfidelity" class="col-sm-8"> <div class="title">Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency</div> <div class="author"> <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <em>Jose Luis Ponton</em>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em> , Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/animation_fidelity_ieeevr23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we study the impact of the avatar’s animation fidelity on different tasks. We compare three animation techniques: two of them using Inverse Kinematics to reconstruct the pose from six trackers, and a third one using a motion capture system with 17 inertial sensors. Our results show that the animation quality affects the Sense of Embodiment. Inertial-based MoCap performs significantly better in mimicking body poses. Surprisingly, IK-based solutions using fewer sensors outperformed MoCap in tasks requiring accurate positioning, which we attribute to the higher latency and the positional drift of the end-effectors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yun2023animationfidelity</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yun, Haoran and Ponton, Jose Luis and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE}}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{286-296}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2642-5254}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/VR55154.2023.00044}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmvr-480.webp 480w,/assets/img/publication_preview/mmvr-800.webp 800w,/assets/img/publication_preview/mmvr-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/mmvr.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmvr.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2022mmvr" class="col-sm-8"> <div class="title">Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices</div> <div class="author"> <em>Jose Luis Ponton</em>, <a href="https://haoranyun.com/" rel="external nofollow noopener" target="_blank">Haoran Yun</a>, <a href="https://www.cs.upc.edu/~virtual/home/index.html" rel="external nofollow noopener" target="_blank">Carlos Andujar</a>, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <em>In ACM SIGGRAPH / Eurographics Symposium on Computer Animation</em> <div class="periodical"> <em>Computer Graphics Forum</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/mmvr_sca2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/UPC-ViRVIG/MMVR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://upc-virvig.github.io/MMVR/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The animation of user avatars plays a crucial role in conveying their pose, gestures, and relative distances to virtual objects or other users. Consumer-grade VR devices typically include three trackers: the Head Mounted Display (HMD) and two handheld VR controllers. Since the problem of reconstructing the user pose from such sparse data is ill-defined, especially for the lower body, the approach adopted by most VR games consists of assuming the body orientation matches that of the HMD, and applying animation blending and time-warping from a reduced set of animations. Unfortunately, this approach produces noticeable mismatches between user and avatar movements. In this work we present a new approach to animate user avatars for current mainstream VR devices. First, we use a neural network to estimate the user’s body orientation based on the tracking information from the HMD and the hand controllers. Then we use this orientation together with the velocity and rotation of the HMD to build a feature vector that feeds a Motion Matching algorithm. We built a MoCap database with animations of VR users wearing a HMD and used it to test our approach on both self-avatars and other users’ avatars. Our results show that our system can provide a large variety of lower body animations while correctly matching the user orientation, which in turn allows us to represent not only forward movements but also stepping in any direction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ponton2022mmvr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Yun, Haoran and Andujar, Carlos and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM SIGGRAPH / Eurographics Symposium on Computer Animation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Graphics Forum}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{41}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{107-118}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1467-8659}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/cgf.14628}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-4 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/avatargo-480.webp 480w,/assets/img/publication_preview/avatargo-800.webp 800w,/assets/img/publication_preview/avatargo-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/avatargo.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="avatargo.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ponton2022avatargo" class="col-sm-8"> <div class="title">AvatarGo: Plug and Play self-avatars for VR</div> <div class="author"> <em>Jose Luis Ponton</em>, Eva Monclus, and <a href="https://www.cs.upc.edu/~npelechano/" rel="external nofollow noopener" target="_blank">Nuria Pelechano</a> </div> <div class="periodical"> <em>In Eurographics 2022 - Short Papers</em> , May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/avatarGo_shortEG2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/UPC-ViRVIG/AvatarGo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The use of self-avatars in a VR application can enhance presence and embodiment which leads to a better user experience. In collaborative VR it also facilitates non-verbal communication. Currently it is possible to track a few body parts with cheap trackers and then apply IK methods to animate a character. However, the correspondence between trackers and avatar joints is typically fixed ad-hoc, which is enough to animate the avatar, but causes noticeable mismatches between the user’s body pose and the avatar. In this paper we present a fast and easy to set up system to compute exact offset values, unique for each user, which leads to improvements in avatar movement. Our user study shows that the Sense of Embodiment increased significantly when using exact offsets as opposed to fixed ones. We also allowed the users to see a semitransparent avatar overlaid with their real body to objectively evaluate the quality of the avatar movement with our technique.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ponton2022avatargo</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Eurographics 2022 - Short Papers}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{AvatarGo: Plug and Play self-avatars for VR}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ponton, Jose Luis and Monclus, Eva and Pelechano, Nuria}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Eurographics Association}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1017-4656}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-03868-169-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.2312/egs.20221037}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Jose Luis Ponton. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-MM0JZRGDZ0"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-MM0JZRGDZ0");</script> </body> </html>